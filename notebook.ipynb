{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune LLaMa on Koyeb\n",
    "\n",
    "### Step 0: Install dependencies and login with HuggingFace and WandB\n",
    "\n",
    "A popup will appear after running the cell below. You need to paste your HuggingFace API token and your WandB API key. Make sure you have requested access to [LLaMa 3.1 8B Instruct](meta-llama/Meta-Llama-3.1-8B-Instruct) on HuggingFace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install llama-recipes ipywidgets wandb > /dev/null\n",
    "\n",
    "import huggingface_hub\n",
    "huggingface_hub.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Optionally, you can login with WandB to track the model's loss during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "wandb.login()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Load the Model\n",
    "\n",
    "In this step, we load the model and tokenizer from the HuggingFace Hub. We define the training config and the hyperparameters we'll use to fine-tune the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "from llama_recipes.configs import train_config as TRAIN_CONFIG\n",
    "\n",
    "train_config = TRAIN_CONFIG()\n",
    "train_config.model_name = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"\n",
    "train_config.num_epochs = 1\n",
    "train_config.run_validation = False\n",
    "train_config.gradient_accumulation_steps = 4\n",
    "train_config.batch_size_training = 1\n",
    "train_config.lr = 3e-4\n",
    "train_config.use_fast_kernels = True\n",
    "train_config.use_fp16 = True\n",
    "train_config.context_length = 4096\n",
    "train_config.batching_strategy = \"packing\"\n",
    "train_config.output_dir = \"Meta-Llama-3.1-8B-Instruct-Apple-MLX\"\n",
    "\n",
    "from transformers import BitsAndBytesConfig\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True,\n",
    ")\n",
    "\n",
    "model = LlamaForCausalLM.from_pretrained(\n",
    "    train_config.model_name,\n",
    "    device_map=\"auto\",\n",
    "    quantization_config=config,\n",
    "    use_cache=False,\n",
    "    attn_implementation=\"sdpa\" if train_config.use_fast_kernels else None,\n",
    "    torch_dtype=torch.float16,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(train_config.model_name)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Test Model Generation\n",
    "\n",
    "Let's now generate some text using our non-fine-tuned model to see how it performs. We'll ask it to explain how to compute the Fast Fourier Transform operation in MLX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"You are a helpful AI coding assistant with expert knowledge of Apple's latest machine learning framework: MLX. You can help answer questions about MLX, provide code snippets, and help debug code.\"\n",
    "\n",
    "def complete_chat(model, tokenizer, messages, **kwargs):\n",
    "    inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", return_dict=True, add_generation_prompt=True).to(model.device)\n",
    "    num_input_tokens = len(inputs[\"input_ids\"][0])\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        return tokenizer.decode(model.generate(**inputs, **kwargs)[0][num_input_tokens:], skip_special_tokens=True)\n",
    "\n",
    "\n",
    "def complete_chat_single_turn(model, tokenizer, user: str, **kwargs):\n",
    "    return complete_chat(model, tokenizer, [\n",
    "        {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n",
    "        {\"role\": \"user\", \"content\": user},\n",
    "    ], **kwargs)\n",
    "\n",
    "print(complete_chat_single_turn(model, tokenizer, \"How do I compute the fast fourrier transform for a signal in MLX?\", max_new_tokens=128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, the model has no knowledge of Apple MLX. It hallucinates a response because Apple MLX was released in December of 2023 which corresponds to LLaMa 3.1's training data cutoff. Hence, the model has seen very few samples related to Apple MLX during training.\n",
    "\n",
    "### Step 3: Define and Load our Custom Dataset\n",
    "\n",
    "Let's load the dataset we've built in the previous steps. We'll define a custom function to process each sample in our dataset, tokenize it and return samples in the format expected by the model. You can read through the comments in the code below to understand how samples are processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_recipes.data.concatenator import ConcatDataset\n",
    "from llama_recipes.utils.config_utils import get_dataloader_kwargs\n",
    "from llama_recipes.utils.dataset_utils import DATASET_PREPROC, get_preprocessed_dataset\n",
    "from copy import deepcopy\n",
    "from dataclasses import dataclass\n",
    "import datasets\n",
    "import torch\n",
    "\n",
    "\n",
    "# We define our custom dataset preprocessing function. It loads our dataset from the Hub,\n",
    "# tokenizes each sample according to LLaMa 3.1's chat template and masks the loss for the\n",
    "# system prompt and the user prompt.\n",
    "def get_apple_mlx_qa_dataset(dataset_config, tokenizer, split_name):\n",
    "    dataset = datasets.load_dataset(\n",
    "        \"koyeb/Apple-MLX-QA\", split=\"train\" if split_name == \"train\" else \"test\"\n",
    "    )\n",
    "\n",
    "    def apply_chat_template(sample):\n",
    "        return {\n",
    "            \"input_ids\": tokenizer.apply_chat_template(\n",
    "                [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": SYSTEM_PROMPT,\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"user\",\n",
    "                        \"content\": sample[\"question\"],\n",
    "                    },\n",
    "                    {\n",
    "                        \"role\": \"assistant\",\n",
    "                        \"content\": sample[\"answer\"],\n",
    "                    },\n",
    "                ],\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=False,\n",
    "            )\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(\n",
    "        apply_chat_template,\n",
    "        remove_columns=list(dataset.features),  # type: ignore\n",
    "    )\n",
    "\n",
    "    def create_labels_with_mask(sample):\n",
    "        labels = deepcopy(sample[\"input_ids\"])\n",
    "\n",
    "        # The EOT token marks the end of a turn in a conversation.\n",
    "        # In our case, the first EOT comes after the system prompt, the second\n",
    "        # after the user prompt, and the third after the assistant answer.\n",
    "        # > [system prompt] EOT [user prompt] EOT [assistant answer] EOT\n",
    "        eot = tokenizer.convert_tokens_to_ids(\"<|eot_id|>\")\n",
    "        indices = [i for i, token in enumerate(sample[\"input_ids\"]) if token == eot]\n",
    "        assert len(indices) == 3, f\"{len(indices)} != 3. {sample['input_ids']}\"\n",
    "\n",
    "        # Mask the loss for the system prompt and the user prompt. We don't want\n",
    "        # the model to predict the question, only the answer.\n",
    "        labels[0 : indices[1] + 1] = [-100] * (indices[1] + 1)\n",
    "        assert len(labels) == len(\n",
    "            sample[\"input_ids\"]\n",
    "        ), f\"{len(labels)} != {len(sample['input_ids'])}\"\n",
    "\n",
    "        return {\"labels\": labels}\n",
    "\n",
    "    dataset = dataset.map(create_labels_with_mask)\n",
    "\n",
    "    def convert_to_tensors(sample):\n",
    "        return {\n",
    "            \"input_ids\": torch.LongTensor(sample[\"input_ids\"]),\n",
    "            \"labels\": torch.LongTensor(sample[\"labels\"]),\n",
    "            \"attention_mask\": torch.tensor([1] * len(sample[\"labels\"])),\n",
    "        }\n",
    "\n",
    "    dataset = dataset.map(convert_to_tensors)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "# To use a custom dataset with LLaMa Recipes, you need to define a custom dataclass\n",
    "# that contains information about the dataset.\n",
    "@dataclass\n",
    "class apple_mlx_qa_dataset:\n",
    "    dataset: str =  \"apple_mlx_qa_dataset\"\n",
    "    train_split: str = \"train\"\n",
    "    test_split: str = \"test\"\n",
    "    trust_remote_code: bool = False\n",
    "\n",
    "# Then, you need to register the dataset preprocessing function in the `DATASET_PREPROC` dictionary.\n",
    "DATASET_PREPROC[\"apple_mlx_qa_dataset\"] = get_apple_mlx_qa_dataset\n",
    "\n",
    "# Finally, we define a utility function to create a PyTorch dataloader from a split of our dataset.\n",
    "def get_dataloader(tokenizer, dataset_config, train_config, split: str = \"train\"):\n",
    "    dataset = get_preprocessed_dataset(tokenizer, dataset_config, split)\n",
    "    dl_kwargs = get_dataloader_kwargs(train_config, dataset, tokenizer, split)\n",
    "    \n",
    "    if split == \"train\" and train_config.batching_strategy == \"packing\":\n",
    "        dataset = ConcatDataset(dataset, chunk_size=train_config.context_length)\n",
    "\n",
    "    # Create data loader\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        num_workers=train_config.num_workers_dataloader,\n",
    "        pin_memory=True,\n",
    "        **dl_kwargs,\n",
    "    )\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "train_dataloader = get_dataloader(tokenizer, apple_mlx_qa_dataset, train_config, \"train\")\n",
    "eval_dataloader = get_dataloader(tokenizer, apple_mlx_qa_dataset, train_config, \"test\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Prepare Model for Paramater-Efficient-Fine-Tuning (PEFT)\n",
    "\n",
    "We can use the `peft` library from HuggingFace to train only a subset of the parameters of our model. This helps with reducing the training time and memory requirements. Furthermore, it can prevent catastrophic forgetting which is a phenomenon where the model forgets what it learned during pre-training when fine-tuning on a new dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import get_peft_model, prepare_model_for_kbit_training, LoraConfig\n",
    "from dataclasses import asdict\n",
    "from llama_recipes.configs import lora_config as LORA_CONFIG\n",
    "\n",
    "lora_config = LORA_CONFIG()\n",
    "lora_config.r = 8\n",
    "lora_config.lora_alpha = 32\n",
    "lora_dropout: float = 0.01\n",
    "\n",
    "peft_config = LoraConfig(**asdict(lora_config))\n",
    "\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "model = get_peft_model(model, peft_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Fine-Tune the Model\n",
    "\n",
    "We'll now fine-tune the model on our custom dataset. We'll use the `train` function from LLaMa Recipes and pass in our model and dataloader. If you have logged in with WandB, you will be able to track the training process on the [WandB dashboard](https://wandb.ai)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from llama_recipes.utils.train_utils import train\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "\n",
    "wandb_run = wandb.init(project=\"finetune-llama-on-koyeb\")\n",
    "wandb_run.config.update(train_config)\n",
    "\n",
    "model.train()\n",
    "\n",
    "optimizer = optim.AdamW(\n",
    "    model.parameters(),\n",
    "    lr=train_config.lr,\n",
    "    weight_decay=train_config.weight_decay,\n",
    ")\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=train_config.gamma)\n",
    "\n",
    "# Start the training process\n",
    "results = train(\n",
    "    model,\n",
    "    train_dataloader,\n",
    "    eval_dataloader,\n",
    "    tokenizer,\n",
    "    optimizer,\n",
    "    scheduler,\n",
    "    train_config.gradient_accumulation_steps,\n",
    "    train_config,\n",
    "    None,\n",
    "    None,\n",
    "    None,\n",
    "    wandb_run,\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Check Model Generation\n",
    "\n",
    "Now that the fine-tuning is complete, let's see if our model is able to help us compute the Fast Fourier Transform operation in MLX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(complete_chat_single_turn(model, tokenizer, \"How do I compute the fast fourrier transform for a signal in MLX?\", max_new_tokens=128))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome! It now uses the correct API for MLX. You can experiment with different prompts, evaluate the model's performance and try to identify gaps in its knowledge.\n",
    "\n",
    "### Step 7: Push Model to HuggingFace Hub\n",
    "\n",
    "If you're happy with the model's outputs, you can save it to the HuggingFace Hub. This will allow you to share it with the community and use it in your projects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_model_name = \"Meta-Llama-3.1-8B-Instruct-Apple-MLX-Adapter\"\n",
    "hf_org = input(\"Enter the HuggingFace organization you want to push the model to: \")\n",
    "model.push_to_hub(f\"{hf_org}/{hf_model_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This will push only the LORA adapater's weights to the HuggingFace Hub (about ~10MB). This is enough to use the model in your Python code, however, it's more convenient to merge the LORA adapter with the base model. You can do this by running the code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = model.merge_and_unload()\n",
    "model_name = \"Meta-Llama-3.1-8B-Instruct-Apple-MLX\"\n",
    "model.push_to_hub(f\"{hf_org}/{model_name}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  },
  "vscode": {
   "interpreter": {
    "hash": "2d58e898dde0263bc564c6968b04150abacfd33eed9b19aaa8e45c040360e146"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
