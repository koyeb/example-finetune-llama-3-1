mlx.optimizers.Adamax
*********************

class Adamax(learning_rate: float | Callable[[array], array], betas: List[float] = [0.9, 0.999], eps: float = 1e-08)

   The Adamax optimizer, a variant of Adam based on the infinity norm
   [1].

   Our Adam implementation follows the original paper and omits the
   bias correction in the first and second moment estimates. In
   detail,

   [1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic
   optimization. ICLR 2015.

      m_{t+1} &= \beta_1 m_t + (1 - \beta_1) g_t \\ v_{t+1} &=
      \max(\beta_2 v_t, |g_t|) \\ w_{t+1} &= w_t - \lambda
      \frac{m_{t+1}}{v_{t+1} + \epsilon}

   Parameters:
      * **learning_rate** (*float** or **callable*) -- The learning
        rate \lambda.

      * **betas** (*Tuple**[**float**, **float**]**, **optional*) --
        The coefficients (\beta_1, \beta_2) used for computing running
        averages of the gradient and its square. Default: "(0.9,
        0.999)"

      * **eps** (*float**, **optional*) -- The term \epsilon added to
        the denominator to improve numerical stability. Default:
        "1e-8"

   -[ Methods ]-

   +------------+--------------------------------------------------------------------------------------------+
   | "__init__  |                                                                                            |
   | "(learnin  |                                                                                            |
   | g_rate[,   |                                                                                            |
   | betas,     |                                                                                            |
   | eps])      |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
   | "apply_si  | Performs the Adamax parameter update and stores v and m in the optimizer state.            |
   | ngle"(gra  |                                                                                            |
   | dient,     |                                                                                            |
   | parameter, |                                                                                            |
   | state)     |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
   | "init_sin  | Initialize optimizer state                                                                 |
   | gle"(para  |                                                                                            |
   | meter,     |                                                                                            |
   | state)     |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
