mlx.optimizers.Adam
*******************

class Adam(learning_rate: float | Callable[[array], array], betas: List[float] = [0.9, 0.999], eps: float = 1e-08)

   The Adam optimizer [1].

   Our Adam implementation follows the original paper and omits the
   bias correction in the first and second moment estimates. In
   detail,

   [1]: Kingma, D.P. and Ba, J., 2015. Adam: A method for stochastic
   optimization. ICLR 2015.

      m_{t+1} &= \beta_1 m_t + (1 - \beta_1) g_t \\ v_{t+1} &= \beta_2
      v_t + (1 - \beta_2) g_t^2 \\ w_{t+1} &= w_t - \lambda
      \frac{m_{t+1}}{\sqrt{v_{t+1} + \epsilon}}

   Parameters:
      * **learning_rate** (*float** or **callable*) -- The learning
        rate \lambda.

      * **betas** (*Tuple**[**float**, **float**]**, **optional*) --
        The coefficients (\beta_1, \beta_2) used for computing running
        averages of the gradient and its square. Default: "(0.9,
        0.999)"

      * **eps** (*float**, **optional*) -- The term \epsilon added to
        the denominator to improve numerical stability. Default:
        "1e-8"

   -[ Methods ]-

   +------------+--------------------------------------------------------------------------------------------+
   | "__init__  |                                                                                            |
   | "(learnin  |                                                                                            |
   | g_rate[,   |                                                                                            |
   | betas,     |                                                                                            |
   | eps])      |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
   | "apply_si  | Performs the Adam parameter update and stores v and m in the optimizer state.              |
   | ngle"(gra  |                                                                                            |
   | dient,     |                                                                                            |
   | parameter, |                                                                                            |
   | state)     |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
   | "init_sin  | Initialize optimizer state                                                                 |
   | gle"(para  |                                                                                            |
   | meter,     |                                                                                            |
   | state)     |                                                                                            |
   +------------+--------------------------------------------------------------------------------------------+
